{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Perform Stemming for text = \"studies studying cries cry\". Compare the results generated with Lemmatization. Comment on your answer how Stemming and Lemmatization differ from each other."
      ],
      "metadata": {
        "id": "MKtTH3BdIDwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re"
      ],
      "metadata": {
        "id": "zVuTGrQJG0gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL4DrOvuFmco",
        "outputId": "0beb533d-600f-43ff-efc6-dc4bf5844868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"studies studying cries cry.\""
      ],
      "metadata": {
        "id": "vy_Cd1cPGbnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentence Tokenization\n",
        "from nltk.tokenize import sent_tokenize\n",
        "tokenized_text= sent_tokenize(text)\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlOTKysRHMs4",
        "outputId": "24ee7fda-4268-48aa-f7ca-0902ac617189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['studies studying cries cry.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokenized_word=word_tokenize(text)\n",
        "print(tokenized_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82gVYBOwHRfw",
        "outputId": "cb303028-51d6-42b5-8aa2-3931dd61f402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['studies', 'studying', 'cries', 'cry', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print stop words of English\n",
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "print(stop_words)\n",
        "text= re.sub('[^a-zA-Z]', ' ',text)\n",
        "tokens = word_tokenize(text.lower())\n",
        "filtered_text=[]\n",
        "for w in tokens:\n",
        "  if w not in stop_words:\n",
        "    filtered_text.append(w)\n",
        "print(\"Tokenized Sentence:\",tokens)\n",
        "print(\"Filterd Sentence:\",filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx0BHhz2HVLw",
        "outputId": "35a222ff-9f4f-4f0e-93ae-95e14d80dfed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'their', 'll', 'couldn', 'some', 'can', 'this', 'above', \"mustn't\", \"you've\", 'your', 'now', 'am', 'didn', 'itself', 'just', 'been', 'myself', 'has', 'below', 'and', 'which', 'them', 'under', 'but', 'theirs', 'haven', 'while', 'shan', 'wasn', \"it's\", 'further', 'each', 'ours', 'y', \"you're\", 'or', 'in', 'own', \"shouldn't\", 'more', 'against', \"needn't\", 'himself', 'the', 'too', 'are', 'him', \"don't\", \"won't\", 'by', 'were', 'will', \"shan't\", 'most', 'with', 'from', 'such', 'to', 'how', \"doesn't\", 't', 'yours', 'than', 'until', \"weren't\", 'his', 'had', 'those', 'did', 'what', 's', 'it', 'o', 'isn', 'into', 'there', 'off', 'our', 'very', 've', 'should', 'hadn', 'up', \"isn't\", 'on', 'again', 'my', 'me', 'have', 'mustn', 'nor', 'for', \"she's\", 'hers', 'all', 'that', 'whom', 'any', 'm', \"didn't\", \"you'd\", 'he', 'other', 'weren', 'who', 'don', 'only', 'as', 'ma', 'both', 'yourself', 'after', 'then', 'between', 'so', 'mightn', 'once', 'about', 'these', \"aren't\", 'an', 'if', 'down', 'aren', \"wouldn't\", \"should've\", 'its', 'same', 'they', 'out', 'why', \"wasn't\", 'through', 'be', 'doesn', 'needn', 'not', 'was', \"mightn't\", 'during', 'no', \"hasn't\", 'do', 'doing', 'she', 're', 'few', 'we', 'over', 'you', 'ain', \"couldn't\", 'her', \"haven't\", 'd', 'when', 'wouldn', 'having', 'herself', 'before', 'at', 'shouldn', 'does', 'being', 'yourselves', \"that'll\", \"you'll\", 'a', 'won', 'of', \"hadn't\", 'themselves', 'is', 'ourselves', 'here', 'i', 'because', 'where', 'hasn'}\n",
            "['studies', 'studying', 'cries', 'cry']\n",
            "Tokenized Sentence: ['studies', 'studying', 'cries', 'cry']\n",
            "Filterd Sentence: ['studies', 'studying', 'cries', 'cry']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps =PorterStemmer()\n",
        "for w in filtered_text:\n",
        "  rootWord=ps.stem(w)\n",
        "  print(rootWord)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wa3v_TMfIfPh",
        "outputId": "6f36f8c4-289b-4e93-c9e8-f9a336f09357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "studi\n",
            "studi\n",
            "cri\n",
            "cri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "text = \"studies studying cries cry\"\n",
        "tokenization = nltk.word_tokenize(text)\n",
        "for w in tokenization:\n",
        "  print(\"Lemma for {} is {}\".format(w,wordnet_lemmatizer.lemmatize(w)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1Rxzw9BKCga",
        "outputId": "dce44798-48e8-40d2-adea-b85fa849ee17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemma for studies is study\n",
            "Lemma for studying is studying\n",
            "Lemma for cries is cry\n",
            "Lemma for cry is cry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write Python code for removing stop words from the below documents, conver the documents into lowercase and calculate the TF, IDF and TFIDF score for each document.\n",
        "\n",
        "documentA = 'Jupiter is the largest Planet'\n",
        "\n",
        "documentB = 'Mars is the fourth planet from the Sun'"
      ],
      "metadata": {
        "id": "Sim01PnSN0By"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "mWz0YkBbNvW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentA = 'Jupiter is the largest Planet'\n",
        "documentB = 'Mars is the fourth planet from the Sun'"
      ],
      "metadata": {
        "id": "aIJiy_cVOC5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentA = documentA.lower()\n",
        "documentB = documentB.lower()"
      ],
      "metadata": {
        "id": "AmMmwtxpWsEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "376eY0T9W8LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentA = [word for word in documentA.split() if word not in stop_words]\n",
        "documentB = [word for word in documentB.split() if word not in stop_words]"
      ],
      "metadata": {
        "id": "DkMYbgzxXUoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_tf = {word: documentA.count(word) for word in documentA}\n",
        "B_tf = {word: documentB.count(word) for word in documentB}"
      ],
      "metadata": {
        "id": "wfhygcQ2Xr3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = set(documentA).union(set(documentB))"
      ],
      "metadata": {
        "id": "xiQayCY8YaXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = {}\n",
        "for word in vocabulary:\n",
        "    df[word] = sum(1 for document in [documentA, documentB] if word in document)"
      ],
      "metadata": {
        "id": "CfSx64CoZHqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the inverse document frequency (idf) for each term in the vocabulary\n",
        "idf = {}\n",
        "for word in vocabulary:\n",
        "    idf[word] = math.log((len([documentA, documentB]) + 1) / (df[word] + 1)) + 1\n",
        "\n"
      ],
      "metadata": {
        "id": "Mk1y63ppZS9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the tf-idf score for each term in each document\n",
        "A_tfidf = {word: A_tf[word] * idf[word] for word in A_tf}\n",
        "B_tfidf = {word: B_tf[word] * idf[word] for word in B_tf}\n"
      ],
      "metadata": {
        "id": "wW7UpTJUZVs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Document A:\")\n",
        "print(\"TF:\", A_tf)\n",
        "print(\"IDF:\", idf)\n",
        "print(\"TF-IDF:\", A_tfidf)\n",
        "\n",
        "print(\"Document B:\")\n",
        "print(\"TF:\", B_tf)\n",
        "print(\"IDF:\", idf)\n",
        "print(\"TF-IDF:\", B_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI2wghUsZYr2",
        "outputId": "8105b524-4c5f-4397-a1b4-8843ebb54833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document A:\n",
            "TF: {'jupiter': 1, 'largest': 1, 'planet': 1}\n",
            "IDF: {'sun': 1.4054651081081644, 'fourth': 1.4054651081081644, 'largest': 1.4054651081081644, 'mars': 1.4054651081081644, 'planet': 1.0, 'jupiter': 1.4054651081081644}\n",
            "TF-IDF: {'jupiter': 1.4054651081081644, 'largest': 1.4054651081081644, 'planet': 1.0}\n",
            "Document B:\n",
            "TF: {'mars': 1, 'fourth': 1, 'planet': 1, 'sun': 1}\n",
            "IDF: {'sun': 1.4054651081081644, 'fourth': 1.4054651081081644, 'largest': 1.4054651081081644, 'mars': 1.4054651081081644, 'planet': 1.0, 'jupiter': 1.4054651081081644}\n",
            "TF-IDF: {'mars': 1.4054651081081644, 'fourth': 1.4054651081081644, 'planet': 1.0, 'sun': 1.4054651081081644}\n"
          ]
        }
      ]
    }
  ]
}