{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f5a330",
   "metadata": {},
   "source": [
    "7) Text Analytics\n",
    "1. Extract Sample document and apply following document preprocessing methods: \n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of documents by calculating Term Frequency and Inverse \n",
    "DocumentFrequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "553e4a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb5c0469",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Prathamesh\n",
      "[nltk_data]     Patil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Prathamesh\n",
      "[nltk_data]     Patil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Prathamesh\n",
      "[nltk_data]     Patil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Prathamesh Patil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc9ec411",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural language processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language. It involves the development of algorithms and models to understand, analyze, and generate human language.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55981bff",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "624cc06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language.', 'It involves the development of algorithms and models to understand, analyze, and generate human language.']\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentencetokenize=sent_tokenize(text)\n",
    "print(sentencetokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4894c3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language', '.', 'it', 'involves', 'the', 'development', 'of', 'algorithms', 'and', 'models', 'to', 'understand', ',', 'analyze', ',', 'and', 'generate', 'human', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "#word tokeniztion\n",
    "from nltk.tokenize import word_tokenize\n",
    "wordtoken=word_tokenize(text.lower())\n",
    "print(wordtoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f25d33",
   "metadata": {},
   "source": [
    "Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea5f470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'nlp', 'subfield', 'artificial', 'intelligence', 'focuses', 'interaction', 'computers', 'humans', 'natural', 'language', 'involves', 'development', 'algorithms', 'models', 'understand', 'analyze', 'generate', 'human', 'language']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "symbols=['(',')',',','.']\n",
    "filteredtext=[]\n",
    "for i in wordtoken:\n",
    "    if i not in stop_words and i not in symbols:\n",
    "        filteredtext.append(i)\n",
    "    \n",
    "print(filteredtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2f9a62",
   "metadata": {},
   "source": [
    "Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12870329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'nlp', 'subfield', 'artifici', 'intellig', 'focus', 'interact', 'comput', 'human', 'natur', 'languag', 'involv', 'develop', 'algorithm', 'model', 'understand', 'analyz', 'gener', 'human', 'languag']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "stemmed=[]\n",
    "for i in filteredtext:\n",
    "    stemmed.append(ps.stem(i))\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102fec56",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d62e845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'nlp', 'subfield', 'artificial', 'intelligence', 'focus', 'interaction', 'computer', 'human', 'natural', 'language', 'involves', 'development', 'algorithm', 'model', 'understand', 'analyze', 'generate', 'human', 'language']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl=WordNetLemmatizer()\n",
    "lemmatized=[]\n",
    "for i in filteredtext:\n",
    "    lemmatized.append(wnl.lemmatize(i))\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5917bd43",
   "metadata": {},
   "source": [
    "POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71b2b2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural: JJ\n",
      "language: NN\n",
      "processing: NN\n",
      "(: (\n",
      "nlp: JJ\n",
      "): )\n",
      "is: VBZ\n",
      "a: DT\n",
      "subfield: NN\n",
      "of: IN\n",
      "artificial: JJ\n",
      "intelligence: NN\n",
      "that: WDT\n",
      "focuses: VBZ\n",
      "on: IN\n",
      "the: DT\n",
      "interaction: NN\n",
      "between: IN\n",
      "computers: NNS\n",
      "and: CC\n",
      "humans: NNS\n",
      "through: IN\n",
      "natural: JJ\n",
      "language: NN\n",
      ".: .\n",
      "it: PRP\n",
      "involves: VBZ\n",
      "the: DT\n",
      "development: NN\n",
      "of: IN\n",
      "algorithms: NN\n",
      "and: CC\n",
      "models: NNS\n",
      "to: TO\n",
      "understand: VB\n",
      ",: ,\n",
      "analyze: NN\n",
      ",: ,\n",
      "and: CC\n",
      "generate: VB\n",
      "human: JJ\n",
      "language: NN\n",
      ".: .\n"
     ]
    }
   ],
   "source": [
    "pos_tags=[]\n",
    "pos_tags.extend(nltk.pos_tag(wordtoken))\n",
    "for word, pos_tag in pos_tags:\n",
    "    print(f\"{word}: {pos_tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb618506",
   "metadata": {},
   "source": [
    "TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b1d9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tf={word:filteredtext.count(word)/len(filteredtext) for word in filteredtext}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6452633b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natural': 0.09090909090909091,\n",
       " 'language': 0.13636363636363635,\n",
       " 'processing': 0.045454545454545456,\n",
       " 'nlp': 0.045454545454545456,\n",
       " 'subfield': 0.045454545454545456,\n",
       " 'artificial': 0.045454545454545456,\n",
       " 'intelligence': 0.045454545454545456,\n",
       " 'focuses': 0.045454545454545456,\n",
       " 'interaction': 0.045454545454545456,\n",
       " 'computers': 0.045454545454545456,\n",
       " 'humans': 0.045454545454545456,\n",
       " 'involves': 0.045454545454545456,\n",
       " 'development': 0.045454545454545456,\n",
       " 'algorithms': 0.045454545454545456,\n",
       " 'models': 0.045454545454545456,\n",
       " 'understand': 0.045454545454545456,\n",
       " 'analyze': 0.045454545454545456,\n",
       " 'generate': 0.045454545454545456,\n",
       " 'human': 0.045454545454545456}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8e59331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'natural': 0.4054651081081644, 'language': 0.28768207245178085, 'processing': 0.6931471805599453, 'nlp': 0.6931471805599453, 'subfield': 0.6931471805599453, 'artificial': 0.6931471805599453, 'intelligence': 0.6931471805599453, 'focuses': 0.6931471805599453, 'interaction': 0.6931471805599453, 'computers': 0.6931471805599453, 'humans': 0.6931471805599453, 'involves': 0.6931471805599453, 'development': 0.6931471805599453, 'algorithms': 0.6931471805599453, 'models': 0.6931471805599453, 'understand': 0.6931471805599453, 'analyze': 0.6931471805599453, 'generate': 0.6931471805599453, 'human': 0.6931471805599453}\n"
     ]
    }
   ],
   "source": [
    "noofdocuments=1\n",
    "IDF={word:math.log(noofdocuments/filteredtext.count(word)+1) for word in filteredtext}\n",
    "print(IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a49d5141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'natural': 0.036860464373469494, 'language': 0.03922937351615193, 'processing': 0.03150669002545206, 'nlp': 0.03150669002545206, 'subfield': 0.03150669002545206, 'artificial': 0.03150669002545206, 'intelligence': 0.03150669002545206, 'focuses': 0.03150669002545206, 'interaction': 0.03150669002545206, 'computers': 0.03150669002545206, 'humans': 0.03150669002545206, 'involves': 0.03150669002545206, 'development': 0.03150669002545206, 'algorithms': 0.03150669002545206, 'models': 0.03150669002545206, 'understand': 0.03150669002545206, 'analyze': 0.03150669002545206, 'generate': 0.03150669002545206, 'human': 0.03150669002545206}\n"
     ]
    }
   ],
   "source": [
    "tfidf={word:Tf[word]*IDF[word] for word in filteredtext}\n",
    "print(tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
